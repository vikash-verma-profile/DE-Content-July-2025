questionTitle,questionType,questionDescription,correctOptionNumbers,positiveMarks,negativeMarks,questionAnswerExplanation,questionOption1,questionOption2,questionOption3,questionOption4,questionOption5,questionOption6,questionOption7,questionOption8
DLTYamlAnatomy,direct,Which field in a DLT YAML pipeline specifies the cluster configuration for the pipeline?,3,1,0,"The `clusters` block defines the cluster settings such as node type, autoscaling, and policy.",name,channel,clusters,edition,,,,
AutoLoaderBasics,direct,What is the role of Auto Loader in Databricks DLT pipelines?,2,1,0,Auto Loader incrementally reads files from cloud storage and detects new files automatically.,It loads data using JDBC connections,It incrementally loads and processes new files from cloud storage,It requires schema to be static,It works only with CSV files,,,,
ExpectRules,direct,What happens when an `EXPECT` rule fails with the `drop` action in a DLT pipeline?,4,1,0,Rows failing the rule are silently dropped from processing.,The pipeline fails immediately,Rows are quarantined to a separate table,Pipeline logs an alert but continues,Failing rows are dropped silently,,,,
CDCIngestPattern,direct,Which of the following statements is true about CDC ingestion in DLT pipelines?,3,1,0,CDC ingest uses metadata columns like `_change_type` to apply changes correctly to the target table.,CDC works only for streaming sources,CDC requires full refresh on each batch,CDC uses metadata columns like `_change_type`,CDC requires manual table recreation,,,,
MonitoringAndHooks,direct,What is the purpose of event hooks in a DLT pipeline?,1,1,0,Event hooks trigger external actions such as Slack notifications or Datadog logging based on pipeline status.,They control job scheduling,They transform the input schema,They reduce pipeline latency,They add metadata to logs,,,,
SchemaEvolution,direct,"In Delta Live Tables, what happens when new columns appear in the source schema during Auto Loader ingestion?",2,1,0,"With schema evolution enabled, DLT automatically adds the new columns to the target table.",They are ignored silently,They are added automatically if schema evolution is enabled,Pipelines fail due to schema mismatch,They overwrite the table schema,,,,
PromoteEnvs,direct,Which of the following is a recommended practice when promoting DLT pipelines across environments?,3,1,0,"Using parameters and environment tags helps manage differences between dev, staging, and prod.",Rewriting YAML manually for each environment,Keeping separate pipelines with hardcoded paths,Using parameterization and env tags,Disabling validation during promotion,,,,
PipelineMonitoring,direct,Which view provides the most detailed metrics and lineage for DLT pipelines?,4,1,0,"The Delta Live Tables UI includes a DAG with lineage, task runtimes, and success/failure tracking.",Spark UI,SQL History tab,DLT Pipeline logs,DLT UI with DAG view,,,,
YamlComponents,direct,Which component of the DLT YAML defines data quality rules?,4,1,0,The `expectations` block defines data quality constraints and rules.,clusters,channel,target,expectations,,,,
DLTGovACLs,direct,"In Unity Catalog, which feature allows defining fine-grained table-level access?",3,1,0,"ACLs can be applied at the catalog, schema, and table levels to enforce access control.",Catalog tags,Dynamic views,Table ACLs,Notebook policies,,,,
